{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "186e03bd-88ac-428c-8368-aedcfc101cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install rank_bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d4f099a-ef51-4b5c-a435-30225fb79ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1af12e39-951f-429b-90a4-3389f33271bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m pip install -U spacy\n",
    "#!python -m pip install scispacy \n",
    "#!python -m pip install \"https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.1/en_ner_bionlp13cg_md-0.5.1.tar.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "968e7594-39c0-4e5f-9e3c-43f022dfd6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mandatory imports\n",
    "import time\n",
    "import re\n",
    "from Bio import Entrez\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import urllib.request, urllib.error, urllib.parse\n",
    "import json\n",
    "#used for visualizations\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "#used for ranking and tokenization\n",
    "from rank_bm25 import BM25Okapi\n",
    "import spacy \n",
    "import scispacy \n",
    "import en_ner_bionlp13cg_md #The model we are going to use from spacy import displacy from scispacy.abbreviation\n",
    "from scispacy.abbreviation import AbbreviationDetector \n",
    "from scispacy.umls_linking import UmlsEntityLinker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13c8df16-9197-41d2-9e4a-62e8795c3c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "## Thesaurus declaration\n",
    "thesaurus = {\n",
    "    \"Ivermectin\" : [\"IVM\", \"Ivermectin\", \"Avermectin\", \"avermectin\", \"ivermectin\", \"stromectol\", \"Stromectol\", \"Eqvalan\", \"Ivomec\", \"Mectizan\", \"Dihydroavermectin\", \"MK 933\", \"MK-933\", \"MK933\",\"MK-0933\", \"L 640471\", \"L-640471\", \"C48H74O14\", \"IV\", \"IVM-654\", \"IVR-25\", \"IV-104\", \"IVE-11\", \"IVER-15\"],\n",
    "    \"GABA\" : [\"GABA\", \"GABAergic\", \"gamma-aminobutyric acid\"],\n",
    "    \"Zebrafish\" : [\"Zebrafish\", \"Danio rerio\"],\n",
    "    \"COVID-19\" : [\"COVID-19\", \"COVID\", \"SARS-CoV-2\"],\n",
    "    \"Glutamate\" : [\"Glutamate\", \"glutamate\",\"Glu\", \"L-(+)-glutamate\",\"L-Glu\", \"L-Glutamate\", \"L-glutamate\", \"L glutamate\", \"L glutamate\"]\n",
    "}\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0758957-98e0-47b0-bfa7-cc356b6cab88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def global_db_search(query_list, genes_list=[]):\n",
    "\n",
    "    if len(genes_list) != 0:\n",
    "        for i in query_list:\n",
    "            for j in genes_list:\n",
    "                query = i + j + \"[tiab]\"\n",
    "\n",
    "                handle = Entrez.egquery(term=query)\n",
    "                record = Entrez.read(handle)\n",
    "                df = pd.DataFrame(record[\"eGQueryResult\"]).head(2)\n",
    "                df[\"Query\"] = query\n",
    "                append_data(df, 'global_query_res.csv', False)\n",
    "                time.sleep(0.34)\n",
    "    else:\n",
    "        for i in query_list:\n",
    "            handle = Entrez.egquery(term=i)\n",
    "            record = Entrez.read(handle)\n",
    "            df = pd.DataFrame(record[\"eGQueryResult\"]).head(2)\n",
    "            df[\"Query\"] = i\n",
    "            append_data(df, 'global_query_res.csv', False)\n",
    "            time.sleep(0.34)\n",
    "    return\n",
    "\n",
    "\n",
    "## Function for reading in the df \"summary\" results\n",
    "def read_in_results(file_name):\n",
    "\n",
    "    # The converters are there so that each list is NOT inside a string\n",
    "    res_df = pd.read_csv(file_name,  converters={\"MainID_List\": lambda x: x.strip(\"[]\").replace(\"'\", \"\").split(\", \"),\n",
    "                                                    \"P_Dates\": lambda x: x.strip(\"[]\").replace(\"'\", \"\").split(\", \"),\n",
    "                                                    \"P_Years\": lambda x: x.strip(\"[]\").replace(\"'\", \"\").split(\", \"),\n",
    "                                                    \"LinkedID_List\": lambda x: x.strip(\"[]\").replace(\"'\", \"\").split(\", \"),\n",
    "                                                    \"Query_Count\": int})\n",
    "    return res_df\n",
    "\n",
    "\n",
    "def esummary_info(in_webenv_key, in_query_key, db_name):\n",
    "\n",
    "    # Obtaining DocSums for a set of IDs that are stored on the Entrez History server.\n",
    "    handle = Entrez.esummary(db=db_name, webenv=in_webenv_key, query_key=in_query_key)\n",
    "    record = Entrez.read(handle)\n",
    "\n",
    "    publ_dates, publ_years = get_published_dates(record)\n",
    "\n",
    "    if db_name == \"pubmed\":\n",
    "        ids_list = get_pmcids(record)\n",
    "    else:\n",
    "        ids_list = get_pmids(record)\n",
    "\n",
    "    return publ_dates, publ_years, ids_list\n",
    "\n",
    "\n",
    "def get_published_dates(esummary_rec):\n",
    "\n",
    "    retr_dates = []\n",
    "    retr_years = []\n",
    "    check = True\n",
    "    i = 0\n",
    "    for article in esummary_rec:\n",
    "        # \"PubDate\" is often of the form: '2021 Nov 26'\n",
    "        retr_dates.append(article[\"PubDate\"])\n",
    "        date = article[\"PubDate\"].split()\n",
    "        while(i < 2):\n",
    "            if len(date[i]) == 4:\n",
    "                p_year = int(date[i])\n",
    "                i = 3\n",
    "            else:\n",
    "                i = i + 1\n",
    "        #p_year = int(article[\"PubDate\"].split()[0])\n",
    "        #p_year = int(article[\"P_Years\"].split()[0])\n",
    "        #changed from PubDate to P_Years because of error when changing search term\n",
    "        retr_years.append(p_year)\n",
    "\n",
    "    return retr_dates, retr_years\n",
    "\n",
    "\n",
    "def get_pmcids(esummary_rec):\n",
    "\n",
    "    pmcids_list = []\n",
    "    for i in esummary_rec:\n",
    "        # If \"pmc\" is there, then this article also has a PMCID (i.e., it's also found in the PubMed Central db)\n",
    "        if \"pmc\" in i[\"ArticleIds\"]:\n",
    "            pmcids_list.append(i[\"ArticleIds\"][\"pmc\"])\n",
    "        else:\n",
    "            pmcids_list.append(np.NaN)\n",
    "\n",
    "    return pmcids_list\n",
    "\n",
    "\n",
    "def get_pmids(esummary_rec):\n",
    "\n",
    "    pmids_list = []\n",
    "    for i in esummary_rec:\n",
    "        # '0' means that the article has no PMID (i.e., it's not found in the PubMed db)\n",
    "        if i[\"ArticleIds\"][\"pmid\"] == '0':\n",
    "            pmids_list.append(np.NaN)\n",
    "        else:\n",
    "            pmids_list.append(i[\"ArticleIds\"][\"pmid\"])\n",
    "\n",
    "    return pmids_list\n",
    "\n",
    "\n",
    "## Function that retrieves summary results from a given set of queries (which don't require a gene list)\n",
    "def get_query_info_no_genes(query_in, db_name):\n",
    "\n",
    "    # relevance: Records are sorted based on relevance to your search. (Relevance ranking)\n",
    "    search_results = Entrez.read(\n",
    "        Entrez.esearch(db=db_name, term=query_in, sort=\"relevance\", retmax=5000, usehistory=\"y\")\n",
    "        )\n",
    "\n",
    "    # NEED TO FIRST CHECK IF WE GOT ANY RESULTS FROM THAT QUERY\n",
    "    if len(search_results[\"IdList\"]) == 0:\n",
    "        print(\"No Results.\")\n",
    "        return\n",
    "    else:\n",
    "        # With search_results, we will use its WebEnv value and QueryKey value\n",
    "        p_dates, p_years, ids_list = esummary_info(search_results[\"WebEnv\"], search_results[\"QueryKey\"], db_name)\n",
    "\n",
    "        time.sleep(0.34)\n",
    "\n",
    "        return pd.DataFrame([[query_in, db_name, search_results['Count'], search_results['IdList'], p_dates, p_years, ids_list]],\n",
    "                                columns=['Query', 'Db_Name', 'Query_Count', 'MainID_List', 'P_Dates', 'P_Years', 'LinkedID_List'])\n",
    "\n",
    "\n",
    "## Function that retrieves summary results from a given set of queries (which requires a gene list)\n",
    "def get_query_info(query_in, genes, db_name):\n",
    "\n",
    "    gene_query = []\n",
    "    query = \"\"\n",
    "\n",
    "    for i in genes:\n",
    "        # Example of db_name values in this use case: \"pubmed\" or \"pmc\"\n",
    "        if db_name == \"pubmed\":\n",
    "            # PubMed's Search field tag: Title/Abstract [tiab]\n",
    "            query = query_in + i + \"[tiab]\"\n",
    "        else:\n",
    "            query = query_in + i\n",
    "\n",
    "        # relevance: Records are sorted based on relevance to your search. (Relevance ranking)\n",
    "        search_results = Entrez.read(\n",
    "            Entrez.esearch(db=db_name, term=query, sort=\"relevance\", retmax=5000, usehistory=\"y\")\n",
    "            )\n",
    "     \n",
    "        # NEED TO FIRST CHECK IF WE GOT ANY RESULTS FROM THAT QUERY\n",
    "        if len(search_results[\"IdList\"]) == 0:\n",
    "            continue\n",
    "\n",
    "        # With search_results, we will use its WebEnv value and QueryKey value\n",
    "        p_dates, p_years, ids_list = esummary_info(search_results[\"WebEnv\"], search_results[\"QueryKey\"], db_name)\n",
    "       \n",
    "        gene_query.append([query, db_name, search_results['Count'], search_results['IdList'], p_dates, p_years, ids_list])\n",
    "        time.sleep(0.34)\n",
    "          \n",
    "    return pd.DataFrame(gene_query, columns=['Query', 'Db_Name', 'Query_Count', 'MainID_List', 'P_Dates', 'P_Years', 'LinkedID_List'])\n",
    "\n",
    "\n",
    "## Function for obtaining citation counts for the set of IDs found in the \"summary\" df\n",
    "def cited_cnt_table(df_summary, db_name):\n",
    "\n",
    "    elink_data = []\n",
    "    link_name = \"\"\n",
    "\n",
    "    if db_name == \"pubmed\":\n",
    "        link_name = \"pubmed_pubmed_citedin\"\n",
    "    else:\n",
    "        link_name = \"pmc_pmc_citedby\"  # \"pmc\" is the other db_name in this use case\n",
    "\n",
    "    for i in range(0, len(df_summary)):\n",
    "\n",
    "        query_term = df_summary.iloc[i][\"Query\"]\n",
    "\n",
    "        for id_num in df_summary.iloc[i][\"MainID_List\"]:\n",
    "\n",
    "            record = Entrez.read(Entrez.elink(id=id_num, dbfrom=db_name, db=db_name, linkname=link_name))\n",
    "         \n",
    "            if len(record[0][\"LinkSetDb\"]) != 0:\n",
    "                cited_counts = len(record[0][\"LinkSetDb\"][0][\"Link\"])\n",
    "            else:\n",
    "                # 'LinkSetDb' key contains empty list when an article has no citation counts\n",
    "                cited_counts = 0\n",
    "            elink_data.append([query_term, db_name, id_num, cited_counts])\n",
    "\n",
    "            if (df_summary.iloc[i][\"MainID_List\"].index(id_num) + 1) % 3 == 0:\n",
    "                time.sleep(0.34)\n",
    "\n",
    "    return pd.DataFrame(elink_data, columns=[\"Query\", \"Db_Name\", \"Id_List\", \"Citation_Cnts\"]) \n",
    "\n",
    "\n",
    "## Function that returns the Top-k results (pass in k as an argument to the function, input by the user)\n",
    "def get_top_k(df, k_val):\n",
    "\n",
    "    q_top_k = []\n",
    "\n",
    "    for q in df[\"Query\"].unique():\n",
    "        matches_ids = []  # For each query version, these are the IDs meeting the criteria of having citation counts >= 25\n",
    "        counts = []\n",
    "        df_temp = df[df[\"Query\"] == q]\n",
    "\n",
    "        for i in range(0, len(df_temp)):\n",
    "            if df_temp.iloc[i][\"Citation_Cnts\"] >= 25:\n",
    "                matches_ids.append(int(df_temp.iloc[i][\"Id_List\"]))\n",
    "                counts.append(df_temp.iloc[i][\"Citation_Cnts\"])\n",
    "                if len(matches_ids) == k_val:\n",
    "                    break\n",
    "        if len(matches_ids) == 0:\n",
    "            continue\n",
    "        q_top_k.append([q, matches_ids, counts])\n",
    "\n",
    "    top_k_df = pd.DataFrame(q_top_k, columns=[\"Query\", \"Top_\"+str(k_val)+\"_Ids\", \"Citation_Cnts\"])\n",
    "\n",
    "    return top_k_df\n",
    "\n",
    "\n",
    "## Function that appends DataFrame rows to a CSV file\n",
    "def append_data(df, file_name, is_new_file):\n",
    "\n",
    "    if is_new_file:\n",
    "        # if True, then\n",
    "        df.to_csv(file_name, index=False)\n",
    "    else:\n",
    "        # False: This is an existing CSV file\n",
    "        df.to_csv(file_name, mode='a', index=False, header=False)\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15ad664-00ed-45c2-b82d-6bdb63797f7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\danie\\AppData\\Roaming\\Python\\Python38\\site-packages\\spacy\\util.py:877: UserWarning: [W095] Model 'en_core_web_sm' (3.3.0) was trained with spaCy v3.3 and may not be 100% compatible with the current version (3.4.2). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "# Load in the model for English\n",
    "old_nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp = spacy.load(\"en_ner_bionlp13cg_md\")\n",
    "# Can't retrieve XML if you don't have a query.\n",
    "query = \"GABA AND Glutamate\"\n",
    "Entrez.email = \"n01365801@unf.edu.com\"\n",
    "df_q_pubmed = get_query_info_no_genes(query, \"pubmed\")\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0a4fb4-3e76-49d6-8137-e30c4b853bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Documentation for fetch_pubmed()\n",
    "\n",
    "This function's purpose is to use NCBI's E-Utils to get the body of articles, given an id.\n",
    "The E-Util used in E-Fetch.\n",
    "Future Work on this could be adjusting the argument, to allow for just a list of IDs, instead of a Pandas DataFrame slice.\n",
    "\n",
    "Arguments:\n",
    "    * ids: The IDs of articles from a Pandas DataFrame\n",
    "Return Value: A list of the records.\n",
    "\"\"\"\n",
    "def fetch_pubmed(ids): \n",
    "    i = 0\n",
    "    records_pubmed = []\n",
    "    # Fetch all records pertaining to our queries.\n",
    "    for row in ids:\n",
    "        for uid in row:\n",
    "            i = i + 1\n",
    "            try:\n",
    "                handle = Entrez.efetch(db=\"pubmed\", id=uid, rettype='', retmode='xml')\n",
    "            except:\n",
    "                print(\"Fetch Failure at index\" + str(i))\n",
    "            try:\n",
    "                record = Entrez.read(handle, validate=True, escape=True)\n",
    "                records_pubmed.append(record)\n",
    "            except:\n",
    "                print(\"Read Failure at index\" + str(i))\n",
    "                \n",
    "\n",
    "    # Be polite and flush/close the stream like a good programmer.\n",
    "    handle.close()\n",
    "    return records_pubmed\n",
    "print(\"Done.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761df95c-3b9b-4ced-be54-94bbf29f704b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_q_pubmed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2a3a7c-60ce-4871-8fae-4aa2651ea715",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fetch failures due to Http error when the API is particularly busy. Errors reduce when the query is run during off hours.\n",
    "records_pubmed = fetch_pubmed(df_q_pubmed['MainID_List'])\n",
    "print(str(len(records_pubmed)) + ' records.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfcd7d35-d972-4502-8db5-6cbc884061da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Documentation for invert_dict()\n",
    "\n",
    "This function's sole purpose is to invert a dictionary, so that the values of the old are the keys of the new,\n",
    "and the keys of the old are the values of the new, in list format.\n",
    "\n",
    "Arguments:\n",
    "    dictionary: The dictionary to be inverted.\n",
    "Return Value: The inverted dictionary following the above design.\n",
    "\"\"\"\n",
    "def invert_dict(dictionary):\n",
    "    dict_inverted = {} # output\n",
    "    for (k, v) in dictionary.items():\n",
    "        if v in dict_inverted.keys():\n",
    "            dict_inverted[v].append(k)\n",
    "        else:\n",
    "            dict_inverted[v] = [k]\n",
    "            \n",
    "    return dict_inverted\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c85a474-be4d-4df1-b1ff-07e58c039be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Documentation for clean_text()\n",
    "\n",
    "This function intakes a text string and removes punctuation that does not signal the end of a sentence.\n",
    "\n",
    "Arguments:\n",
    "    text: a string of text\n",
    "Return Value: \n",
    "    clean_string: a string without punctuation\n",
    "\"\"\"\n",
    "def clean_text(text):\n",
    "    try:\n",
    "        #text_alphanumeric = re.sub(r'[^a-zA-Z0-9]', ' ', text)\n",
    "        clean_string = re.sub(r'[^\\w\\s]', '', text)\n",
    "        return clean_string\n",
    "    except:\n",
    "        print(\"error\")\n",
    "        return None\n",
    "    \n",
    "\"\"\"\n",
    "Documentation for stopword_remove()\n",
    "\n",
    "This function intakes a text string and removes english stopwords from nlp.Defaults.stop_words.\n",
    "\n",
    "Arguments:\n",
    "    text: a string of text\n",
    "Return Value: \n",
    "    clean_string: a string without stopwords\n",
    "\"\"\"\n",
    "\n",
    "def stopword_remove(text):\n",
    "    text_for_removal = text.split(\" \")\n",
    "    tokens_filtered= [word for word in text_for_removal if not word in nlp.Defaults.stop_words]\n",
    "    return ' '.join(tokens_filtered)\n",
    "\n",
    "\"\"\"\n",
    "Documentation for stopword_remove()\n",
    "\n",
    "This function is used as a preprocessing pipeline to call the clean_text() and stopword_remove() functions.\n",
    "\n",
    "Arguments:\n",
    "    text: a string of text\n",
    "Return Value: \n",
    "   final_text: a string without english stopwords and non-stopping punctuation\n",
    "\"\"\"\n",
    "\n",
    "def preprocess(text):\n",
    "    text_no_punct = clean_text(text)\n",
    "    final_text = stopword_remove(text_no_punct)\n",
    "    return final_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529df7e7-04f5-4f14-8c7e-d9162e97c46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Documentation for find_comentions()\n",
    "\n",
    "Future Work for this function includes generalizing it to be able to handle both PubMed and PMC.\n",
    "This will likely require some work on reranking() below, as it only handles PubMed formatted XML,\n",
    "due to issues with PMC and Biopython present while writing this code.\n",
    "\n",
    "Arguments:\n",
    "    * thes: A dictionary containing the synonyms of query terms.\n",
    "    * doc: A SpaCy Doc object that contains the text we are looking at.\n",
    "Return Value: A tuple in the form of (sentences, proximity list)\n",
    "\"\"\"\n",
    "def find_comentions(thes, doc):\n",
    "    j = 0\n",
    "    sentences = []\n",
    "    proximity_list = []\n",
    "    count = 0\n",
    "    #print(doc)\n",
    "    for sentence in doc.sents:\n",
    "        #print(sentence)\n",
    "        prev_term = \"\"\n",
    "        term_seen = False\n",
    "        first_i = 0\n",
    "        for word in sentence:\n",
    "            for term in thes.keys():\n",
    "                #print(term)\n",
    "                if term_seen:\n",
    "                    if (word.text in thes[term]) and (word.text not in thes[prev_term]):\n",
    "                        count = count + 1\n",
    "                        #print(word.text + \"Calculated!!\")\n",
    "                        #print(word.i)\n",
    "                        proximity_list.append(int(word.i - first_i))\n",
    "                        first_i = word.i\n",
    "                        prev_term = term\n",
    "                        if j not in sentences:\n",
    "                            sentences.append(j)\n",
    "                        break\n",
    "                elif (word.text in thes[term]):\n",
    "                    count = count + 1\n",
    "                    #print(word.text)\n",
    "                    #print(word.i)\n",
    "                    term_seen = True\n",
    "                    first_i = word.i\n",
    "                    prev_term = term\n",
    "                    break\n",
    "        j= j+1\n",
    "    return (sentences, proximity_list, count)\n",
    "\n",
    "def extract_comentions(doc, sentences):\n",
    "    j = 0\n",
    "    comention_sentences = []\n",
    "    for sentence in doc.sents:\n",
    "        if j in sentences and sentence.text not in comention_sentences:\n",
    "            comention_sentences.append(str(sentence.text))\n",
    "        j = j + 1\n",
    "    return str(comention_sentences)\n",
    "            \n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f163243-45b5-4959-bd4e-80bee48d802a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Documentation for bm_reranking()\n",
    "\n",
    "This function will use find_comentions(), invert_dict(), and bm25 Okapi to create a ranking of the articles.\n",
    "The articles will be identified by UID.\n",
    "\n",
    "Future additions to this reranking function include using Ms. Victoria's get_top_k() and cited_cnt_table() functions to add in the\n",
    "25 citation requirement for credibility. Other work includes breaking out some functionality into other functions in order to clean up the mess.\n",
    "\n",
    "Arguments:\n",
    "    * records: A list of XML Objects returned by E-Fetch.\n",
    "    * query_terms: A list of query_terms. These then get selected out of the Thesaurus.\n",
    "Return Value: \n",
    "    rankings: The rankings for the records. Type is a Pandas DataFrame.\n",
    "    bmtop15: Top 15 ranked records.\n",
    "    top_100: Top 100 of the rankings.\n",
    "\"\"\"\n",
    "def bm_reranking(records, query_terms):\n",
    "    # snippet from https://stackoverflow.com/questions/29216889/slicing-a-dictionary\n",
    "    # While in the answer, they add a check to make sure the key is in original dict\n",
    "    # (the thesaurus in this case), it is safe to assume that the key is in the dict,\n",
    "    # because the thesaurus should contain all possible terms.\n",
    "    inner_thesaurus = {k:thesaurus[k] for k in query_terms}\n",
    "    #print(inner_thesaurus)\n",
    "    # DataFrame Data\n",
    "    pmids = []\n",
    "    titles = []\n",
    "    abstracts = []\n",
    "    token_abstracts = []\n",
    "    relevancy_score = {}\n",
    "    abstract_score = []\n",
    "    comention_sentences = []\n",
    "    tokenized_queries = []\n",
    "    i = 0\n",
    "    j = 0\n",
    "    for record in records:\n",
    "        # Some records do not have an Abstract (???) so we need to check for an abstract\n",
    "        # TODO: Deal with the articles that don't have abstracts\n",
    "        try:\n",
    "            pmid = str(record['PubmedArticle'][0]['MedlineCitation']['PMID'])\n",
    "            article_keys = record['PubmedArticle'][0]['MedlineCitation']['Article'].keys()\n",
    "            if 'Abstract' in article_keys: # We have an Abstract\n",
    "                # TODO: Stop rewriting this indexing mess every time.\n",
    "                abstract_text = str(record['PubmedArticle'][0]['MedlineCitation']['Article']['Abstract']['AbstractText'])\n",
    "                # Process the abstract\n",
    "                #doc = nlp(abstract_text)\n",
    "                #Preprocess abstracts with punct and stopword removal\n",
    "                abstract_text_clean = re.sub(r'/', ' ', abstract_text) #removes / characters because of unique case involving scispacy NER, combines query1/query2 into an unrecognizable token\n",
    "                abstract_text_clean = stopword_remove(abstract_text_clean)\n",
    "                doc = nlp(abstract_text_clean) \n",
    "                #removes punctuation that does not indicate the end of a sentence\n",
    "                text_no_punct = [token.text for token in doc if not (token.is_punct or token.is_left_punct or token.is_right_punct) or (token.text ==  (\".\" or \"!\" or \"?\"))]\n",
    "                token_abstracts.append(text_no_punct)\n",
    "                \n",
    "                #dataframe collection\n",
    "                pmids.append(pmid)\n",
    "                titles.append(str(record['PubmedArticle'][0]['MedlineCitation']['Article']['ArticleTitle']))\n",
    "                abstract_text = \"\".join(str(i) for i in (record['PubmedArticle'][0]['MedlineCitation']['Article']['Abstract']['AbstractText']))\n",
    "                abstracts.append(abstract_text)\n",
    "        except:\n",
    "            print(\"Record indexed at \" + str(i) + \" PMID not found.\")\n",
    "        i = i + 1\n",
    "        j = j + 1\n",
    "    \n",
    "    bm25 = BM25Okapi(token_abstracts)\n",
    "    \n",
    "    for key in inner_thesaurus.keys():\n",
    "        tokenized_queries = inner_thesaurus[key] + tokenized_queries\n",
    "    \n",
    "    doc_scores = bm25.get_scores(tokenized_queries)\n",
    "   \n",
    "    print(tokenized_queries)\n",
    "    \n",
    "    prerankings = pd.DataFrame(data=[pmids, titles, abstracts, doc_scores]).transpose()\n",
    "    prerankings.columns = [\"PMID\", \"Title\", \"Abstract\", \"BM25 Relevancy Score\"]\n",
    "    bmrankings= prerankings.sort_values(by=[\"BM25 Relevancy Score\"], ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    return bmrankings.iloc[:15].copy(), bmrankings\n",
    "\n",
    "bmtop15,bmrankings = bm_reranking(records_pubmed, [\"GABA\",\"Glutamate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba54f572-b811-433a-b199-7c2e622beef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "bmtop15.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6daa4d9-05cf-4910-8129-b8f3efe61af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_relevancy_score(proximity_count):\n",
    "    if (len(proximity_count) != 0):\n",
    "        relevancy_score = sum([1/count for count in proximity_count])\n",
    "    else:\n",
    "        relevancy_score = 0\n",
    "    return relevancy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27658a40-2d25-470b-9e63-befb4fab9b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Documentation for reranking()\n",
    "\n",
    "This function will use find_comentions(), invert_dict(), and word_proximity() to create a general ranking of the articles.\n",
    "The articles will be identified by UID.\n",
    "\n",
    "Future additions to this reranking function include using Ms. Victoria's get_top_k() and cited_cnt_table() functions to add in the\n",
    "25 citation requirement for credibility. Other work includes breaking out some functionality into other functions in order to clean up the mess.\n",
    "\n",
    "Arguments:\n",
    "    * records: A list of XML Objects returned by E-Fetch.\n",
    "    * query_terms: A list of query_terms. These then get selected out of the Thesaurus.\n",
    "Return Value: \n",
    "    rankings: The top 15 rankings for the records. Type is a Pandas DataFrame.\n",
    "    prerankings: Unsorted scored dataframe of records. Used for comparison to bm25 rankings.\n",
    "    top_100\n",
    "    \n",
    "\"\"\"\n",
    "### TODO: Break apart this function into smaller functions\n",
    "    # Namely, creation of combined_criteria, creation of top_15, and creation of DataFrame\n",
    "def reranking(records, query_terms):\n",
    "    # snippet from https://stackoverflow.com/questions/29216889/slicing-a-dictionary\n",
    "    # While in the answer, they add a check to make sure the key is in original dict\n",
    "    # (the thesaurus in this case), it is safe to assume that the key is in the dict,\n",
    "    # because the thesaurus should contain all possible terms.\n",
    "    inner_thesaurus = {k:thesaurus[k] for k in query_terms}\n",
    "\n",
    "    # DataFrame Data\n",
    "    pmids = []\n",
    "    titles = []\n",
    "    abstracts = []\n",
    "    relevancy_score = {}\n",
    "    comention_sentences = []\n",
    "    raw_counts = []\n",
    "    i = 0\n",
    "    j = 0\n",
    "    for record in records:\n",
    "        # Some records do not have an Abstract (???) so we need to check for an abstract\n",
    "        # TODO: Deal with the articles that don't have abstracts\n",
    "        try:\n",
    "            pmid = str(record['PubmedArticle'][0]['MedlineCitation']['PMID'])\n",
    "            article_keys = record['PubmedArticle'][0]['MedlineCitation']['Article'].keys()\n",
    "            if 'Abstract' in article_keys: # We have an Abstract\n",
    "                # TODO: Stop rewriting this indexing mess every time.\n",
    "                abstract_text = str(record['PubmedArticle'][0]['MedlineCitation']['Article']['Abstract']['AbstractText'])\n",
    "                # Process the abstract\n",
    "                #CHANGE using raw string for comention\n",
    "                \n",
    "                #FIXME find way to stop calling nlp() twice \n",
    "                #tokenize and process abstracts to extract comentions and relevancy scores\n",
    "                abstract_text_clean = re.sub(r'/', ' ', abstract_text) #removes / characters because of unique case involving scispacy NER, combines query1/query2 into an unrecognizable token\n",
    "                doc = nlp(abstract_text_clean) \n",
    "                #removes punctuation that does not indicate the end of a sentence\n",
    "                text_no_punct = [token for token in doc if not (token.is_punct or token.is_left_punct or token.is_right_punct) or (token.text ==  (\".\" or \"!\" or \"?\"))]\n",
    "                text_no_punct= ' '.join(token.text for token in text_no_punct)\n",
    "                \n",
    "                doc_no_punct= nlp(text_no_punct)\n",
    "                (comention_sents_indices, proximity_count, raw_count) = find_comentions(doc=doc_no_punct, thes=inner_thesaurus)\n",
    "                \n",
    "                \n",
    "                # DataFrame data collection\n",
    "                pmids.append(pmid)\n",
    "                relevancy_score[pmid] = calc_relevancy_score(proximity_count)\n",
    "                raw_counts.append(raw_count)\n",
    "                titles.append(str(record['PubmedArticle'][0]['MedlineCitation']['Article']['ArticleTitle']))\n",
    "                try:\n",
    "                    abstract_text = \"\".join(str(i) for i in (record['PubmedArticle'][0]['MedlineCitation']['Article']['Abstract']['AbstractText']))\n",
    "                    doc = nlp(abstract_text)\n",
    "                    comention_sents = extract_comentions(doc, comention_sents_indices)\n",
    "                    abstracts.append(abstract_text)\n",
    "                    comention_sentences.append(comention_sents)\n",
    "                except:\n",
    "                    print(\"Reformatting error at \" + str(i) + \".\")\n",
    "                \n",
    "        except:\n",
    "            print(\"Record indexed at \" + str(i) + \" PMID not found.\")\n",
    "        i = i + 1\n",
    "        j = j + 1\n",
    "    print(\"Num records proccessed: \" + str(j))\n",
    "    \n",
    "    # Relevancy score will be used to calculate the index.\n",
    "    prerankings = pd.DataFrame(data=[pmids, titles, abstracts, relevancy_score.values(), raw_counts, comention_sentences]).transpose()\n",
    "    prerankings.columns = [\"PMID\", \"Title\", \"Abstract\", \"Relevancy Score\", \"Number of Query Terms Found\", \"Comention Sentences\"]\n",
    "    #FIXME: remove plotting\n",
    "    \n",
    "    \n",
    "    \n",
    "    #formatting for output\n",
    "    \n",
    "    relevancy_score_inverted = invert_dict(relevancy_score)\n",
    "    # Gathering the top 15 abstracts based on the relevancy score.\n",
    "    # We conglomerate all of the scores into a single list, flatten it, and then simply slice out the first 15 elements.\n",
    "    top_15 = []\n",
    "    for i in sorted(relevancy_score_inverted.keys(), reverse=True):\n",
    "        top_15.append(relevancy_score_inverted[i])\n",
    "    # This list comprehension is flattening the list of lists produced by combined_criteria filtering.\n",
    "    top_15 = [x for xs in top_15 for x in xs]\n",
    "    top_15 = top_15[:15]\n",
    "    prerankings.astype({'Comention Sentences': 'string'}).dtypes\n",
    "    prerankings[\"Comention Sentences\"] = prerankings[\"Comention Sentences\"].apply(lambda x: str(x).strip(\"[]\"))\n",
    "    \n",
    "    #getting top 100 for word clouds\n",
    "    top_100 = []\n",
    "    for i in sorted(relevancy_score_inverted.keys(), reverse=True):\n",
    "        top_100.append(relevancy_score_inverted[i])\n",
    "    # This list comprehension is flattening the list of lists produced by combined_criteria filtering.\n",
    "    top_100 = [x for xs in top_100 for x in xs]\n",
    "    top_100 = top_100[:100]\n",
    "    \n",
    "    # Now that we have the UIDs of our top 15, we can grab them\n",
    "    #FIXME: need to redo top 15 selections with iloc like function for BMRANKINGS\n",
    "    prerankings = prerankings.sort_values(by=\"Relevancy Score\", ascending=False).reset_index(drop=True)\n",
    "    rankings = prerankings[prerankings[\"PMID\"].isin(top_15)].sort_values(by=\"Relevancy Score\", ascending=False).reset_index(drop=True)\n",
    "    top_100ranked = prerankings[prerankings[\"PMID\"].isin(top_100)].sort_values(by=\"Relevancy Score\", ascending=False).reset_index(drop=True)\n",
    "    return rankings, top_100ranked, prerankings\n",
    "\n",
    "# FIXME: Do NOT hardcode the query_terms, pull them in from the query.\n",
    "rankings, top_100, prerankings = reranking(records_pubmed, [\"GABA\",\"Glutamate\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404d7431-27b6-4067-bd3d-d86ab2853259",
   "metadata": {},
   "outputs": [],
   "source": [
    "prerankings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620907f5-4fb2-4a5b-a1e5-29913147c7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#needed for merge, sets index values for join\n",
    "prerankings[prerankings.index.name] = prerankings.index\n",
    "prerankings[\"Comention Score Rank\"]= prerankings[prerankings.index.name]\n",
    "bmrankings[bmrankings.index.name] = bmrankings.index\n",
    "bmrankings[\"BM25 Rank\"]= bmrankings[bmrankings.index.name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6716a1e-2168-4c87-8a6a-6ddb6d4c9457",
   "metadata": {},
   "outputs": [],
   "source": [
    "bmrankings_merged = pd.merge(left = bmrankings, right = prerankings, left_on = \"PMID\", right_on =\"PMID\")\n",
    "bmrankings_mergedtop15 = bmrankings_merged.iloc[:15]\n",
    "\n",
    "bmrankings_mergedtop15.head()\n",
    "\n",
    "bm_comparison =  bmrankings_mergedtop15.filter(['PMID','Title_x','Abstract_x', 'BM25 Relevancy Score','Relevancy Score', 'Comention Score Rank'], axis=1)\n",
    "bm_comparison.head()\n",
    "\n",
    "bm_comparison.rename(columns={\"Title_x\": \"Title\", \"Abstract_x\": \"Abstract\",\"Relevancy Score\":\"Comention Relevancy Score\"}, inplace = True)\n",
    "bm_comparison.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2602e2-068b-4a8a-ac78-584a26526fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "rankings_merged = pd.merge(left = prerankings, right = bmrankings, left_on = \"PMID\", right_on =\"PMID\")\n",
    "rankings_mergedtop15 = rankings_merged.iloc[:15]\n",
    "\n",
    "rankings_mergedtop15.head()\n",
    "\n",
    "comention_comparison =  rankings_mergedtop15.filter(['PMID','Title_x','Abstract_x', 'Relevancy Score','BM25 Relevancy Score', 'BM25 Rank'], axis=1)\n",
    "comention_comparison.head()\n",
    "\n",
    "comention_comparison.rename(columns={\"Title_x\": \"Title\", \"Abstract_x\": \"Abstract\",\"Comention Relevancy Score\":\"Relevancy Score\"}, inplace = True)\n",
    "comention_comparison.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da838f5c-879d-4c60-84bc-ca492574a00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#output compared rankings to other method\n",
    "bm_comparison.to_csv(\"GABA_AND_GlutamateBM25vsComentionRanks07.csv\")\n",
    "comention_comparison.to_csv(\"ComentionvsBM25Ranks05.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6558c7e9-f2fe-411c-bf2e-83b33e7b5ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "REST_URL = \"http://data.bioontology.org\"\n",
    "\"\"\"\n",
    "Use of this requires a valid API Key.\n",
    "This simply requires setting up an account at bioportal.bioonotology.org,\n",
    "but do note that API Keys are meant to be secret, and probably more permanent than\n",
    "what amounts to basically an intern's key.\n",
    "\"\"\"\n",
    "API_KEY = \"0de90ddd-0da2-42ee-b0a0-2eb3b90dface\"\n",
    "ont = \"HP,MDM\"\n",
    "def get_json(url):\n",
    "    opener = urllib.request.build_opener()\n",
    "    opener.addheaders = [('Authorization', 'apikey token=' + API_KEY)]\n",
    "    return json.loads(opener.open(url).read())\n",
    "\n",
    "abstract = \"Ivermectin (IVM) is an antiparasitic drug that is widely used in domestic animals. In mammals, IVM acts as a γ-aminobutyric acid (GABA) receptor agonist. This neurotransmitter plays an important role in the regulation of female sexual behavior. The present study investigated the effects of therapeutic (0.2 mg/kg) and high (1.0 mg/kg) IVM doses on female sexual behavior in physiological and pharmacological conditions. Female rats in estrus or treated with estradiol valerate to induce sexual behavior 24 h before the experiments were used. Ivermectin was administered 15 min before the sexual observations. The number of lordosis events in 10 mounts was recorded to calculate the lordosis quotient. The intensity of lordosis (0 [no lordosis], 1 [low lordosis], 2 [normal lordosis] and 3 [exaggerated lordosis]) was scored. In estrus and hormonal treated female rats, both IVM doses decreased the intensity of the lordosis reflex and the percentage of females that presented high levels of lordosis (exaggerated lordosis). However, the number of females that presented lordosis was unaltered. We conclude that in both hormonal conditions, 0.2mg/kg IVM treatment reduced female sexual behavior and the execution of the lordosis reflex. The present results may be useful for avoiding the side effects of this drug in veterinary practice.\"\n",
    "annotations = get_json(REST_URL + \"/annotator?text=\" + urllib.parse.quote(abstract) + \"&ontologies=\" + ont)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67472574-1b9f-451e-97d3-bf5be2794b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Documentation for ann_extraction()\n",
    "\n",
    "This function's purpose is to feed the queried abstracts into different bioontology databases for annotation and extract the terms found within.\n",
    "Calls get_annotations() for parsing of results and formatting.\n",
    "\n",
    "Arguments:\n",
    "    * ont: List of codes determining which ontologies to annotate from.\n",
    "    * table: Pandas dataframe of records pulled from query.\n",
    "Return Value: ranked_annotations, a list of dictionaries with annotated terms separated by source ontology that can be set added as columns to the dataframe of choice.\n",
    "\"\"\"\n",
    "def ann_extraction(ont, table):\n",
    "    #i = 0\n",
    "    ranked_annotations = []\n",
    "    ranking = table\n",
    "    for abstract in ranking[\"Abstract\"]:\n",
    "        #i = i +1\n",
    "        #print(\"ABSTRACT \" + str(i))\n",
    "        annotations = (get_json(REST_URL + \"/annotator?text=\" + urllib.parse.quote(abstract) + \"&ontologies=\" \n",
    "                                + ont + \"&roots_only=true&require_exact_match=true&exclude_synonyms=true\"))\n",
    "        ranked_annotations.append(get_annotations(annotations, ont))\n",
    "         \n",
    "    return ranked_annotations\n",
    "    \n",
    "    \n",
    "\"\"\"\n",
    "Documentation for get_annotations()\n",
    "\n",
    "This function takes the returned JSON annotation objects from ann_extraction(), seperates the annotations by source ontology, and formats them for clean output.\n",
    "\n",
    "Arguments:\n",
    "    * ont: List of codes determining which ontologies to annotate from.\n",
    "    * annotations: JSON formatted collection of annotation objects to be parsed for output.\n",
    "Return Value: ont_dict, a dictionary with the ontology codes as keys and a list of annotated terms from the source ontology as values.\n",
    "\"\"\"\n",
    "def get_annotations(annotations, ont):\n",
    "    ont_cat = ont.split(\",\")\n",
    "    ont_dict={}\n",
    "    for ont in ont_cat:\n",
    "        ont_dict[ont] = []\n",
    "\n",
    "    for result in annotations:\n",
    "        class_details = result[\"annotatedClass\"]\n",
    "        try:\n",
    "            class_details = get_json(result[\"annotatedClass\"][\"links\"][\"self\"])\n",
    "        except urllib.error.HTTPError:\n",
    "            print(f\"Error retrieving {result['annotatedClass']['@id']}\")\n",
    "            continue\n",
    "        ont_dict[str(class_details[\"links\"][\"ontology\"].split(\"/\")[-1])] += [class_details[\"prefLabel\"]]\n",
    "        \n",
    "\n",
    "    return ont_dict\n",
    "\"\"\"\n",
    "Documentation for attach_annotations()\n",
    "\n",
    "This function takes the list of separated annotation dictionaries per abstract, checks for duplicates within each abstract, and formats them or attachment to the CSV.\n",
    "\n",
    "Arguments:\n",
    "    * ont: List of codes determining which ontologies to annotate from.\n",
    "    * columns: List of dictionaries with separated annotations by ontology.\n",
    "    * rankings: Dataframe of records from query.\n",
    "Return Value: None\n",
    "\"\"\"\n",
    "#adds anotatons to rankings table\n",
    "def attach_annotations(ont, columns, rankings):\n",
    "\n",
    "    sources = ont.split(\",\")\n",
    "    for source in sources:\n",
    "        cols = []\n",
    "        for ele in columns:\n",
    "            items = []\n",
    "            temp = ele[source]\n",
    "            for i in temp:\n",
    "                if i not in items:\n",
    "                    items.append(i)\n",
    "            items = str(items)\n",
    "            cols.append(items.strip(\"[]\"))\n",
    "        rankings[source] = cols\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88570724-4b91-479e-a5ad-ee1f89579e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "attach_annotations(ont, columns, rankings)\n",
    "attach_annotations(ont, top100_columns, top_100)\n",
    "attach_annotations(ont, bm_columns, bmtop15)\n",
    "columns = ann_extraction(ont, rankings)\n",
    "top100_columns = ann_extraction(ont, top_100)\n",
    "bm_columns = ann_extraction(ont, bmtop15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ed846c-de2f-439e-a6d5-dec2fcd16319",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# CSV output of individual ranakings with annotations\n",
    "rankings.to_csv(\"Glutamate and GABA22scispacy.csv\")\n",
    "bmtop15.to_csv(\"Glutamate and GABA_BMstopwordtest8.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8728495-c70b-4261-8595-221118ae46c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = prerankings[\"Relevancy Score\"].plot(kind = 'hist', logy = True, bins = 120, title = \"Distribution of Relevancy Scores\",\n",
    "                                    xlim = (0,12))\n",
    "ax.set_xlabel(\"Relevancy Scores\")\n",
    "ax.set_ylabel(\"Log Frequency\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014d121f-7383-4b13-84ce-6e55a606eea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_annotations(ont, rankings):\n",
    "    i=0\n",
    "    ont_ls = []\n",
    "    ont_cat = ont.split(\",\")\n",
    "    for cat in ont_cat:\n",
    "        ont_dict = {}\n",
    "        for record in rankings[cat]:\n",
    "            annotations = record.split(\",\")\n",
    "            for item in annotations:\n",
    "                if item != '':\n",
    "                    temp = item.strip(\" \")\n",
    "                    if temp in ont_dict.keys():\n",
    "                        ont_dict[temp] = ont_dict[temp] + 1\n",
    "                    else:\n",
    "                        ont_dict[temp] = 1\n",
    "        ont_ls.append(ont_dict)\n",
    "    return ont_ls\n",
    "\n",
    "ann_dicts = count_annotations(ont, rankings)\n",
    "bm_ann_dicts = count_annotations(ont, bmtop15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949fb660-af8a-4751-810f-fbfe37ce7afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_dicts(ann_dicts):\n",
    "    output = []\n",
    "    for values in ann_dicts:\n",
    "        x=dict(sorted(values.items(), key=lambda item: item[1], reverse = True))\n",
    "        output.append(x)\n",
    "    return output\n",
    "sorted_ann_vals = sort_dicts(ann_dicts)\n",
    "sorted_bm_ann_vals = sort_dicts(bm_ann_dicts)\n",
    "print(sorted_ann_vals)\n",
    "print(sorted_bm_ann_vals)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcdcad9-8009-4390-841d-8706a916222e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_to_table(ont, list_sorted_dict):\n",
    "    sources = ont.split(\",\")\n",
    "    i = 0\n",
    "    dfs = []\n",
    "    for i in range (0,len(sources)):\n",
    "        vals = list_sorted_dict[i]\n",
    "        anns = pd.DataFrame.from_dict(vals, orient = \"index\")\n",
    "        anns.columns= [sources[i] + \" Count\"]\n",
    "        dfs.append(anns)\n",
    "    return dfs\n",
    "\n",
    "def concat_dfs(ont, df_list):\n",
    "    sources = ont.split(\",\")\n",
    "    i = 0\n",
    "    result = df_list[0]\n",
    "    for i in range (0, len(sources)):\n",
    "        try:\n",
    "            result = pd.concat([result, df_list[i+1]], axis=1)\n",
    "        except:\n",
    "            return result\n",
    "    return result\n",
    "co_dfs = dict_to_table(ont, sorted_ann_vals)\n",
    "bm_dfs = dict_to_table(ont, sorted_bm_ann_vals)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91634ac9-cc9f-4060-a96d-e3a9de7c406a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#co_dfs[0][0:25].style\n",
    "#co_dfs[1][0:25].style\n",
    "#bm_dfs[0][0:25].style\n",
    "#bm_dfs[1][0:25].style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a061219-2845-4665-8ac0-e5a58eeb3fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Documentation generate_wordclouds()\n",
    "\n",
    "Uses Wordcloud() to generate wordclouds for the annotated records.\n",
    "\n",
    "Arguments:\n",
    "    * ont: List of ontology codes.\n",
    "    * rankings: Pandas dataframe of ranked items\n",
    "\n",
    "\"\"\"\n",
    "def generate_wordclouds(ont, rankings):\n",
    "    sources = ont.split(\",\")\n",
    "    text = \"\"\n",
    "    \n",
    "    for source in sources:\n",
    "        temp = rankings[source]\n",
    "        for item in temp:\n",
    "            annotations = item.split(\",\")\n",
    "            for element in annotations:\n",
    "                text += \" \" + element.strip(\" ''\")\n",
    "        wordcloud = WordCloud(max_font_size=40, max_words=100, background_color=\"white\", normalize_plurals = True, scale = 15 ).generate(text)\n",
    "        plt.figure()\n",
    "        plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "        wordcloud = WordCloud().generate(text)\n",
    "        \n",
    "# Create and generate a word cloud image:\n",
    "        \n",
    "generate_wordclouds(ont, rankings)\n",
    "generate_wordclouds(ont, bmtop15)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
